{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/lorenz/github/cost-optimal-model/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "from models.scripts.m4 import calc_time_for_config_m4\n",
    "from models.utils import distr_maker, model_distr_split_fn\n",
    "\n",
    "class Scheduler:\n",
    "\n",
    "    def __init__(self, instances) -> None:\n",
    "        instances['busy_cores'] = 0\n",
    "        instances['used_mem_caching'] = 0\n",
    "        instances['used_sto_caching'] = 0\n",
    "        instances['used_mem_spooling'] = 0\n",
    "        instances['used_sto_spooling'] = 0\n",
    "        self.instance_types = instances\n",
    "        columns = instances.columns.to_numpy()\n",
    "        # query end events is heap of tuples\n",
    "        # tuples contain query finish time and used resources\n",
    "        # cpu cores\n",
    "        # mem for caching\n",
    "        # sto for caching\n",
    "        # mem for spooling\n",
    "        # sto for spooling\n",
    "        columns = np.append(columns, [\"query_end_events\"])\n",
    "        self.provisioned_instances = pd.DataFrame(columns=columns)\n",
    "        self.current_time = 0\n",
    "        pass\n",
    "\n",
    "    def calc_time(self, query):\n",
    "        if query[\"arrival_time\"] < self.current_time:\n",
    "            raise ValueError(\"no time traveling queries, please.\")\n",
    "        self.current_time = query[\"arrival_time\"]\n",
    "\n",
    "        suitable_instance_types = suitable_instances(self.instance_types, query)\n",
    "\n",
    "        results_new_instances = calc_time(suitable_instance_types, query)\n",
    "\n",
    "        self.provisioned_instances = self.provisioned_instances.apply(lambda row: self.update_used_resources(row), axis=1)\n",
    "\n",
    "        suitable_provisioned_instances = suitable_instances(self.provisioned_instances, query)\n",
    "\n",
    "        if suitable_provisioned_instances.shape[0] > 0:\n",
    "\n",
    "            results_provisioned_instances = calc_time(suitable_provisioned_instances, query)\n",
    "\n",
    "            return pd.concat([results_new_instances, results_provisioned_instances], keys=[\"new_instances\", \"provisioned_instances\"])\n",
    "        \n",
    "        result = pd.concat([results_new_instances], keys=[\"new_instances\"])\n",
    "\n",
    "        result.index.names = [\"provisioned\", \"internal_id\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def update_used_resources(self, row):\n",
    "        heap = row[\"query_end_events\"] \n",
    "        while len(heap) > 0:\n",
    "            head = heap[0]\n",
    "            if head[0] <= self.current_time:\n",
    "                row[\"busy_cores\"] -= head[1]\n",
    "                row[\"used_mem_caching\"] -= head[2]\n",
    "                row[\"used_sto_caching\"] -= head[3]\n",
    "                row[\"used_mem_spooling\"] -= head[4]\n",
    "                row[\"used_sto_spooling\"] -= head[5]\n",
    "                heapq.heappop(heap)\n",
    "            else:\n",
    "                break\n",
    "        return row\n",
    "    \n",
    "    def schedule(self, query, best):\n",
    "        runtime = best[\"stat_time_sum\"]\n",
    "        id = best.name[1]\n",
    "        query_end_event = (\n",
    "            self.current_time + runtime,\n",
    "            query[\"per_server_cores\"],\n",
    "            best[\"used_mem_caching\"],\n",
    "            best[\"used_sto_caching\"],\n",
    "            best[\"used_mem_spooling\"],\n",
    "            best[\"used_sto_spooling\"]\n",
    "        )\n",
    "        if best.name[0] == \"new_instances\":\n",
    "            instance = self.instance_types.loc[id].copy()\n",
    "\n",
    "            instance[\"busy_cores\"] = query_end_event[1]\n",
    "            instance[\"used_mem_caching\"] = query_end_event[2]\n",
    "            instance[\"used_sto_caching\"] = query_end_event[3]\n",
    "            instance[\"used_mem_spooling\"] = query_end_event[4]\n",
    "            instance[\"used_sto_spooling\"] = query_end_event[5]\n",
    "\n",
    "            instance[\"query_end_events\"] = [query_end_event]\n",
    "\n",
    "            self.provisioned_instances.loc[len(self.provisioned_instances.index)] = instance\n",
    "\n",
    "        elif best.name[0] == \"provisioned_instances\":\n",
    "\n",
    "            self.provisioned_instances.loc[id, \"busy_cores\"] += query_end_event[1]\n",
    "            self.provisioned_instances.loc[id, \"used_mem_caching\"] += query_end_event[2]\n",
    "            self.provisioned_instances.loc[id, \"used_sto_caching\"] += query_end_event[3]\n",
    "            self.provisioned_instances.loc[id, \"used_mem_spooling\"] += query_end_event[4]\n",
    "            self.provisioned_instances.loc[id, \"used_sto_spooling\"] += query_end_event[5]\n",
    "\n",
    "            query_end_events = self.provisioned_instances.loc[id][\"query_end_events\"]\n",
    "            heapq.heappush(query_end_events, (query_end_event))\n",
    "\n",
    "def suitable_instances(instances, query):\n",
    "    number_cores = instances[\"calc_cpu_real\"]\n",
    "    number_busy_cores = instances[\"busy_cores\"]\n",
    "    number_cores_needed = number_busy_cores + query[\"per_server_cores\"]\n",
    "    return instances.loc[number_cores >= number_cores_needed]\n",
    "\n",
    "def calc_time(instances, query):\n",
    "    distr_caching_precomputed = distr_maker(shape=query[\"cache_skew\"], size=query[\"total_reads\"])\n",
    "\n",
    "    distr_cache = model_distr_split_fn(distr_caching_precomputed, query[\"first_read_from_s3\"])\n",
    "\n",
    "    spooling_read_sum = round(query[\"total_reads\"] * query[\"spooling_fraction\"])\n",
    "    spooling_distr = [] if spooling_read_sum < 1 else distr_maker(shape=query[\"spooling_skew\"], size=spooling_read_sum)\n",
    "\n",
    "    scaling = 1\n",
    "\n",
    "    return calc_time_for_config_m4(instances, query, 1, distr_cache, spooling_distr, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('new_instances', 8674)\n",
      "new query number cores:\n",
      "8\n",
      "number cores already used:\n",
      "0\n",
      "max cores:\n",
      "36.0\n",
      "1\n",
      "('new_instances', 8674)\n",
      "new query number cores:\n",
      "1\n",
      "number cores already used:\n",
      "0\n",
      "max cores:\n",
      "36.0\n",
      "2\n",
      "removing used ressources from id:\n",
      "0\n",
      "('new_instances', 8674)\n",
      "new query number cores:\n",
      "36\n",
      "number cores already used:\n",
      "0\n",
      "max cores:\n",
      "36.0\n",
      "3\n",
      "removing used ressources from id:\n",
      "2\n",
      "('provisioned_instances', 2)\n",
      "new query number cores:\n",
      "4\n",
      "number cores already used:\n",
      "0\n",
      "max cores:\n",
      "36.0\n",
      "4\n",
      "('provisioned_instances', 2)\n",
      "new query number cores:\n",
      "8\n",
      "number cores already used:\n",
      "4\n",
      "max cores:\n",
      "36.0\n",
      "5\n",
      "('provisioned_instances', 2)\n",
      "new query number cores:\n",
      "8\n",
      "number cores already used:\n",
      "12\n",
      "max cores:\n",
      "36.0\n"
     ]
    }
   ],
   "source": [
    "from models.utils import calc_cost\n",
    "from preprocessing.instances import instSet_transform\n",
    "from models.const import QUERIES\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    instances = instSet_transform()\n",
    "    #instances.to_csv(\"./input/input_\" + \".csv\")\n",
    "    scheduler = Scheduler(instances)\n",
    "    i = 0\n",
    "    for query in QUERIES:\n",
    "        print(i)\n",
    "        results = scheduler.calc_time(query)\n",
    "        results = [results]\n",
    "        results = calc_cost(results)\n",
    "        #results[0].to_csv(\"./output/output_\" + str(i) + \".csv\")\n",
    "        best = results[0].iloc[0]\n",
    "        scheduler.schedule(query, best)\n",
    "        \n",
    "        i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
